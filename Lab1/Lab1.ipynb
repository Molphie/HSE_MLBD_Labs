{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002b3942-8b07-4a1d-9f02-b84af7381b3c",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe6ad66-a326-4401-81f1-16e37fdb62f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/22 11:05:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/22 11:05:39 WARN TaskSetManager: Stage 0 contains a task of very large size (68756 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/03/22 11:05:46 WARN TaskMemoryManager: Failed to allocate a page (8388592 bytes), try again.\n",
      "25/03/22 11:05:46 WARN TaskMemoryManager: Failed to allocate a page (8388592 bytes), try again.\n",
      "25/03/22 11:05:46 WARN TaskMemoryManager: Failed to allocate a page (8388592 bytes), try again.\n",
      "25/03/22 11:05:46 WARN TaskMemoryManager: Failed to allocate a page (8388592 bytes), try again.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV-файлы записаны в папку orders_output\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OrderHistoryGenerator\").getOrCreate()\n",
    "\n",
    "cities = [\n",
    "    (\"Moscow\", 55.7558, 37.6173), (\"Saint Petersburg\", 59.9343, 30.3351),\n",
    "    (\"Novosibirsk\", 55.0084, 82.9357), (\"Ekaterinburg\", 56.8389, 60.6057),\n",
    "    (\"Simferopol\", 44.5653, 34.0615), (\"Nizhny Novgorod\", 56.2965, 43.9361),\n",
    "    (\"Sevastopol\", 44.3600, 36.3200), (\"Samara\", 53.2415, 50.2212),\n",
    "    (\"Rostov-on-Don\", 47.2357, 39.7015), (\"Ufa\", 54.7348, 55.9578),\n",
    "    \n",
    "]\n",
    "\n",
    "city_dict = {city[0]: city for city in cities}\n",
    "\n",
    "facilities = [\n",
    "    (\"Starik_Hynkalych\", city_dict[\"Simferopol\"]),\n",
    "    (\"Starik_Hynkalych\", city_dict[\"Sevastopol\"]),\n",
    "    (\"Starik_Hynkalych\", city_dict[\"Saint Petersburg\"]),\n",
    "    (\"Pushkin\", city_dict[\"Moscow\"]),\n",
    "    (\"Aziatiq\", random.choice(cities)),\n",
    "    (\"Aziatiq\", random.choice(cities)),\n",
    "    (\"The_byk\", random.choice(cities)),\n",
    "    (\"The_byk\", random.choice(cities)),\n",
    "    (\"The_byk\", random.choice(cities)),\n",
    "    (\"Yumi_Yumi\", city_dict[\"Saint Petersburg\"]),\n",
    "    (\"DODOpizza\", random.choice(cities)),\n",
    "    (\"DODOpizza\", random.choice(cities)),\n",
    "    (\"DODOpizza\", random.choice(cities)),\n",
    "    (\"LETH\", random.choice(cities))\n",
    "]\n",
    "\n",
    "menu_items = [\n",
    "    (\"Pizza\", 500), (\"Tea\", 100), (\"Apple Juice\", 150),\n",
    "    (\"Sushi\", 600), (\"Pasta\", 400), (\"Ramen\", 450),\n",
    "    (\"Salad_Caesar\", 350), (\"Steak\", 800), (\"Harcho\", 300),\n",
    "    (\"Hinkali\", 600), (\"Chebyrek\", 190), (\"Fried_potato\", 200),\n",
    "    (\"Spicy_Jokable\", 1000), (\"Lobster\", 3500)\n",
    "]\n",
    "\n",
    "def generate_orders():\n",
    "    start_date = datetime(2024, 1, 1)\n",
    "    end_date = datetime(2024, 12, 31)\n",
    "    num_orders = 1_000_000\n",
    "    \n",
    "    data = []\n",
    "    for _ in range(num_orders):\n",
    "        order_id = random.randint(10000, 999999)\n",
    "        facility_name, (city, lat, lng) = random.choice(facilities)\n",
    "        order_time = start_date + timedelta(seconds=random.randint(0, (end_date - start_date).total_seconds()))\n",
    "        order_time_str = order_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        if random.random() < 0.1:\n",
    "            if random.random() < 0.5:\n",
    "                lat, lng = -999.999, -999.999\n",
    "            else:\n",
    "                facility_name = None\n",
    "\n",
    "        for _ in range(random.randint(1, 5)):\n",
    "            menu_item, price = random.choice(menu_items)\n",
    "            \n",
    "            if random.random() < 0.1:\n",
    "                menu_item, price = None, None\n",
    "            \n",
    "            data.append((order_id, city, \"Russia\", facility_name, lat, lng, menu_item, price, order_time_str))\n",
    "    \n",
    "    return data\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"Order_id\", IntegerType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"facility_name\", StringType(), True),\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lng\", DoubleType(), True),\n",
    "    StructField(\"menu_item\", StringType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"datetime\", StringType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(generate_orders(), schema)\n",
    "output_path = \"orders_output\"\n",
    "df.repartition(10).write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(f\"CSV-файлы записаны в папку {output_path}\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99ecaf-3330-45ad-b1c6-e100a0484fa4",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119126a8-736d-4666-b41f-efd6c0f7fa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Фильтрация завершена. Данные сохранены в Parquet и Avro.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"OrderProcessing\").getOrCreate()\n",
    "\n",
    "input_path = \"orders_output\"\n",
    "output_parquet = \"orders_parquet\"\n",
    "output_avro = \"orders_avro\"\n",
    "deadletter_path = \"deadletter\"\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(input_path)\n",
    "\n",
    "df = df.withColumn(\"price\", col(\"price\").cast(\"int\")) \\\n",
    "       .withColumn(\"lat\", col(\"lat\").cast(DoubleType())) \\\n",
    "       .withColumn(\"lng\", col(\"lng\").cast(DoubleType())) \\\n",
    "       .withColumn(\"datetime\", to_date(col(\"datetime\")))\n",
    "\n",
    "valid_records = df.filter(\n",
    "    (col(\"facility_name\").isNotNull()) & \n",
    "    (col(\"menu_item\").isNotNull()) & \n",
    "    (col(\"price\").isNotNull()) & \n",
    "    (col(\"lat\") != -999.999) & \n",
    "    (col(\"lng\") != -999.999)\n",
    ")\n",
    "\n",
    "invalid_records = df.subtract(valid_records)\n",
    "\n",
    "invalid_records.write.mode(\"overwrite\").csv(deadletter_path, header=True)\n",
    "\n",
    "valid_records.write.mode(\"overwrite\").partitionBy(\"datetime\", \"city\").parquet(output_parquet)\n",
    "valid_records.write.mode(\"overwrite\").partitionBy(\"datetime\", \"city\").format(\"avro\").save(output_avro)\n",
    "\n",
    "print(\"Фильтрация завершена. Данные сохранены в Parquet и Avro.\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3167c8-6d4d-4384-9fd7-c7e93dd3679a",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192494b-d4e1-4f30-a8b2-59a6f792c18f",
   "metadata": {},
   "source": [
    "CREATE TABLE cities (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name TEXT UNIQUE NOT NULL,\n",
    "    country TEXT NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE facilities (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    city_id INTEGER REFERENCES cities(id) ON DELETE CASCADE\n",
    ");\n",
    "\n",
    "CREATE TABLE orders (\n",
    "    id BIGINT PRIMARY KEY,\n",
    "    facility_id INTEGER REFERENCES facilities(id) ON DELETE CASCADE,\n",
    "    datetime TIMESTAMP NOT NULL\n",
    ");\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    order_id BIGINT REFERENCES orders(id) ON DELETE CASCADE,\n",
    "    menu_item TEXT NOT NULL,\n",
    "    price INTEGER\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b08f90-1611-4a29-8008-e44c05945489",
   "metadata": {},
   "source": [
    "\n",
    "Schema |    Name     | Type  |  Owner   \n",
    "--------+-------------+-------+----------\n",
    " public | cities      | table | mlbduser\n",
    " public | facilities  | table | mlbduser\n",
    " public | order_items | table | mlbduser\n",
    " public | orders      | table | mlbduser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8c7c6eb-40b7-4456-8c4a-0947936e890c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Нет новых заказов для вставки.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены в PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "postgres_jar_path = \"postgresql-42.7.3.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoadOrdersToPostgres\") \\\n",
    "    .config(\"spark.jars\", postgres_jar_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", postgres_jar_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", postgres_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"avro\").load(\"orders_avro\")\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/mldbdb?ssl=false\"\n",
    "properties = {\n",
    "    \"user\": \"mlbduser\",\n",
    "    \"password\": \"mlbdpasswd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "cities_df = df.select(\"city\", \"country\").distinct().withColumnRenamed(\"city\", \"name\")\n",
    "cities_df = cities_df.dropDuplicates([\"name\"])\n",
    "\n",
    "cities_db_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"cities\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "new_cities_df = cities_df.join(cities_db_df, cities_df.name == cities_db_df.name, \"left_anti\")\n",
    "new_cities_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"cities\", mode=\"append\", properties=properties)\n",
    "\n",
    "cities_db_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"cities\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "facilities_df = df.select(\"facility_name\", \"city\").distinct().withColumnRenamed(\"facility_name\", \"name\")\n",
    "\n",
    "facilities_df = facilities_df.alias(\"facilities\")\n",
    "cities_db_df = cities_db_df.alias(\"cities\")\n",
    "\n",
    "facilities_df = facilities_df.join(\n",
    "    cities_db_df,\n",
    "    facilities_df.city == cities_db_df.name\n",
    ").select(\n",
    "    facilities_df[\"name\"],\n",
    "    cities_db_df[\"id\"].alias(\"city_id\")\n",
    ")\n",
    "\n",
    "facilities_df = facilities_df.dropDuplicates([\"name\", \"city_id\"])\n",
    "facilities_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"facilities\", mode=\"append\", properties=properties)\n",
    "\n",
    "facilities_db_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"facilities\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "orders_df = df.select(\"Order_id\", \"facility_name\", \"datetime\").distinct().withColumnRenamed(\"Order_id\", \"id\")\n",
    "\n",
    "orders_df = orders_df.alias(\"orders\")\n",
    "facilities_db_df = facilities_db_df.alias(\"facilities\")\n",
    "\n",
    "orders_df = orders_df.join(\n",
    "    facilities_db_df,\n",
    "    orders_df.facility_name == facilities_db_df.name\n",
    ").select(\n",
    "    orders_df[\"id\"],\n",
    "    facilities_db_df[\"id\"].alias(\"facility_id\"),\n",
    "    orders_df[\"datetime\"]\n",
    ")\n",
    "\n",
    "orders_df = orders_df.filter(col(\"datetime\").isNotNull())\n",
    "orders_df = orders_df.withColumn(\"id\", col(\"id\").cast(\"bigint\"))\n",
    "\n",
    "orders_df = orders_df.dropDuplicates([\"id\"])\n",
    "\n",
    "orders_db_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"orders\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "new_orders_df = orders_df.join(orders_db_df, orders_df.id == orders_db_df.id, \"left_anti\")\n",
    "\n",
    "if new_orders_df.count() > 0:\n",
    "    new_orders_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"orders\", mode=\"append\", properties=properties)\n",
    "    print(f\"Вставлено {new_orders_df.count()} новых заказов.\")\n",
    "else:\n",
    "    print(\"Нет новых заказов для вставки.\")\n",
    "\n",
    "orders_db_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"orders\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "order_items_df = df.select(\"Order_id\", \"menu_item\", \"price\")\n",
    "order_items_df = order_items_df.join(orders_db_df, order_items_df.Order_id == orders_db_df.id).select(\n",
    "    orders_db_df[\"id\"].alias(\"order_id\"),\n",
    "    order_items_df[\"menu_item\"],\n",
    "    order_items_df[\"price\"]\n",
    ")\n",
    "\n",
    "order_items_df = order_items_df.dropDuplicates([\"order_id\", \"menu_item\"])\n",
    "order_items_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"order_items\", mode=\"append\", properties=properties)\n",
    "\n",
    "print(\"Данные успешно загружены в PostgreSQL\")\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4258c1e-8bcf-41c3-9804-34cce021a4de",
   "metadata": {},
   "source": [
    "## TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "844028c4-a608-429c-9dcf-203826e41e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/22 14:31:47 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:31:48 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:31:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:31:49 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:31:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:31:50 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:03 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:13 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "25/03/22 14:32:55 WARN TaskMemoryManager: Failed to allocate a page (134217728 bytes), try again.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вставлено 0 новых заказов.\n",
      "Данные успешно загружены в PostgreSQL (Star Schema)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "postgres_jar_path = \"postgresql-42.7.3.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoadOrdersToPostgresStarSchema\") \\\n",
    "    .config(\"spark.jars\", postgres_jar_path) \\\n",
    "    .config(\"spark.driver.extraClassPath\", postgres_jar_path) \\\n",
    "    .config(\"spark.executor.extraClassPath\", postgres_jar_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.format(\"avro\").load(\"orders_avro\")\n",
    "\n",
    "jdbc_url = \"jdbc:postgresql://localhost:5432/mldbdb?ssl=false\"\n",
    "properties = {\n",
    "    \"user\": \"mlbduser\",\n",
    "    \"password\": \"mlbdpasswd\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "cities_df = df.select(\"city\", \"country\").distinct().withColumnRenamed(\"city\", \"city_name\")\n",
    "cities_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"dim_cities\", mode=\"append\", properties=properties)\n",
    "\n",
    "dim_cities_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"dim_cities\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "facilities_df = df.select(\"facility_name\", \"city\").distinct().withColumnRenamed(\"facility_name\", \"name\")\n",
    "facilities_df = facilities_df.join(dim_cities_df, facilities_df.city == dim_cities_df.city_name) \\\n",
    "    .select(col(\"name\").alias(\"facility_name\"), col(\"city_id\"))\n",
    "\n",
    "facilities_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"dim_facilities\", mode=\"append\", properties=properties)\n",
    "\n",
    "menu_items_df = df.select(\"menu_item\", \"price\").distinct().withColumnRenamed(\"menu_item\", \"menu_item_name\")\n",
    "menu_items_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"dim_menu_items\", mode=\"append\", properties=properties)\n",
    "\n",
    "dim_facilities_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"dim_facilities\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "dim_menu_items_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"dim_menu_items\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "orders_fact_df = df.join(dim_facilities_df, df.facility_name == dim_facilities_df.facility_name) \\\n",
    "    .join(dim_menu_items_df, df.menu_item == dim_menu_items_df.menu_item_name) \\\n",
    "    .select(\n",
    "        col(\"Order_id\").cast(\"bigint\").alias(\"order_id\"),\n",
    "        col(\"facility_id\"),\n",
    "        col(\"menu_item_id\"),\n",
    "        col(\"datetime\"),\n",
    "        lit(1).alias(\"quantity\")\n",
    "    )\n",
    "\n",
    "orders_fact_df = orders_fact_df.dropDuplicates([\"order_id\"])\n",
    "\n",
    "orders_db_df = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"dbtable\", \"orders_fact\") \\\n",
    "    .option(\"user\", \"mlbduser\") \\\n",
    "    .option(\"password\", \"mlbdpasswd\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "new_orders_df = orders_fact_df.join(orders_db_df, orders_fact_df.order_id == orders_db_df.order_id, \"left_anti\")\n",
    "\n",
    "if new_orders_df.count() > 0:\n",
    "    new_orders_df.write.option(\"driver\", \"org.postgresql.Driver\").jdbc(jdbc_url, \"orders_fact\", mode=\"append\", properties=properties)\n",
    "    print(f\"Вставлено {new_orders_df.count()} новых заказов.\")\n",
    "else:\n",
    "    print(\"Нет новых заказов для вставки.\")\n",
    "\n",
    "print(\"Данные успешно загружены в PostgreSQL (Star Schema)\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b946690f-4c67-4568-8f16-1e954f720b2a",
   "metadata": {},
   "source": [
    "## TASK 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "959e2033-2a3d-4fc9-8a6c-e1142e10c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены в MongoDB\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as _sum, collect_list, struct\n",
    "from pymongo import MongoClient\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AggregateOrdersToMongoDB\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "parquet_path = \"orders_parquet\"\n",
    "df = spark.read.parquet(parquet_path)\n",
    "\n",
    "aggregated_df = df.groupBy(\"order_id\").agg(\n",
    "    _sum(\"price\").alias(\"total_amount\"),\n",
    "    collect_list(\n",
    "        struct(\n",
    "            col(\"menu_item\").alias(\"item_name\"),\n",
    "            col(\"price\").alias(\"item_price\")\n",
    "        )\n",
    "    ).alias(\"items\")\n",
    ")\n",
    "\n",
    "mongo_uri = \"mongodb://localhost:27017/\"\n",
    "mongo_db = \"mldbdb\"\n",
    "mongo_collection = \"orders\"\n",
    "\n",
    "aggregated_data = aggregated_df.collect()\n",
    "\n",
    "client = MongoClient(mongo_uri)\n",
    "db = client[mongo_db]\n",
    "collection = db[mongo_collection]\n",
    "\n",
    "for record in aggregated_data:\n",
    "    record_dict = record.asDict()\n",
    "    collection.insert_one(record_dict)\n",
    "\n",
    "print(\"Данные успешно загружены в MongoDB\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d9e1c-68c0-4de4-8b99-f29c040b1e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

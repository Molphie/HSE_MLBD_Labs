{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95b59a7a-03c6-42c9-a813-f527ee0a83cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/25 03:26:05 WARN Utils: Your hostname, Molphie resolves to a loopback address: 127.0.1.1; using 192.168.6.223 instead (on interface enp4s0)\n",
      "25/06/25 03:26:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/25 03:26:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/25 03:26:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/06/25 03:26:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "python_exec = sys.executable\n",
    "os.environ['PYSPARK_PYTHON'] = python_exec\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_exec\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CreditCardFraudExperiments\") \\\n",
    "    .config(\"spark.pyspark.python\", python_exec) \\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", python_exec) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abd894f-96a8-4227-a657-b05f822dcd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.clustering import KMeans, GaussianMixture\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, ClusteringEvaluator\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a46cb42-e5a9-49f8-9191-6961d03facca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data_path = \"creditcard.csv\"\n",
    "df = spark.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09aa4854-4786-4e37-a850-f441cdf4f956",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/25 03:26:11 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 12:===================================================>    (12 + 1) / 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 228225, Test size: 56237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "def stratified_split(df, label_col='Class', train_frac=0.8, seed=42):\n",
    "    fractions = df.select(label_col).distinct().rdd.map(lambda r: (r[0], train_frac)).collectAsMap()\n",
    "    train_df = df.stat.sampleBy(label_col, fractions, seed)\n",
    "    test_df = df.subtract(train_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = stratified_split(df)\n",
    "print(f\"Train size: {train_df.count()}, Test size: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e63169d-863c-4a03-bbaf-bc05bfb5535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = [f\"V{i}\" for i in range(1, 29)] + [\"Amount\", \"Time\"]\n",
    "assembler = VectorAssembler(inputCols=feat_cols, outputCol=\"rawFeatures\")\n",
    "scaler = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83ec352-c373-4f55-971a-113b5636b325",
   "metadata": {},
   "source": [
    "# Супервайзд-классификация (RandomForest)\n",
    "\n",
    "Научиться надёжно отделять нормальные транзакции от мошеннических"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e01385ae-4e61-4a7a-acc1-ab1569d65346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", numTrees=100, maxDepth=6, seed=42)\n",
    "pipeline_clf = Pipeline(stages=[assembler, scaler, rf])\n",
    "model_clf = pipeline_clf.fit(train_df)\n",
    "preds_clf = model_clf.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ac3e69-7fde-4e42-a3d1-dd7d6c761886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Classification Metrics ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.979442015045451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:====>                                                   (1 + 12) / 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PR  AUC: 0.8249789182256847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ev_roc = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "ev_pr  = BinaryClassificationEvaluator(labelCol=\"Class\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderPR\")\n",
    "print(\"== Classification Metrics ==\")\n",
    "print(\"ROC AUC:\", ev_roc.evaluate(preds_clf))\n",
    "print(\"PR  AUC:\", ev_pr.evaluate(preds_clf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db8bed-d889-49b2-af39-8ceb4acb76b1",
   "metadata": {},
   "source": [
    "# Неконтролируемая кластеризация (KMeans)\n",
    "\n",
    "Найти естественные группы транзакций и проверить, есть ли кластер, где фродовых примеров заметно больше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6654ab47-a18a-43f9-98d7-ff14ba5203fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "scaled_df = Pipeline(stages=[assembler, scaler]).fit(df).transform(df)\n",
    "kmeans = KMeans(k=4, seed=42, featuresCol=\"features\", predictionCol=\"cluster\")\n",
    "model_km = kmeans.fit(scaled_df)\n",
    "clusters = model_km.transform(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06ba0319-5e32-41a0-83f8-79b0874cca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Clustering Metrics ==\n",
      "Silhouette: 0.07065676641022597\n",
      "WSSSE: 7707699.754617099\n",
      "+-------+------+------+--------------------+\n",
      "|cluster| total|frauds|          fraud_rate|\n",
      "+-------+------+------+--------------------+\n",
      "|      1|119789|   169|0.001410814014642413|\n",
      "|      3| 43081|   194|0.004503145238039...|\n",
      "|      2|  4451|    10|0.002246686137946...|\n",
      "|      0|117486|   119|0.001012886641812...|\n",
      "+-------+------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "ev_sil = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", metricName=\"silhouette\")\n",
    "silhouette_score = ev_sil.evaluate(clusters)\n",
    "training_cost = model_km.summary.trainingCost\n",
    "print(\"\\n== Clustering Metrics ==\")\n",
    "print(f\"Silhouette: {silhouette_score}\")\n",
    "print(f\"WSSSE: {training_cost}\")\n",
    "clusters.groupBy(\"cluster\").agg(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.sum(\"Class\").alias(\"frauds\")\n",
    ").withColumn(\"fraud_rate\", F.col(\"frauds\")/F.col(\"total\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320cfa1b-2acc-42bb-acdf-9b5b15eb2237",
   "metadata": {},
   "source": [
    "# Детекция аномалий (GaussianMixture)\n",
    "\n",
    "Использовать распределение данных, чтобы найти аномалии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf54c1c0-66b0-4499-b0d3-6f45a1af5b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "pipeline_gmm = Pipeline(stages=[assembler, scaler, GaussianMixture(k=4, featuresCol=\"features\", predictionCol=\"gmm_cluster\", probabilityCol=\"probability\")])\n",
    "model_gmm = pipeline_gmm.fit(df)\n",
    "preds_gmm = model_gmm.transform(df)\n",
    "max_prob_udf = udf(lambda v: float(max(v)), DoubleType())\n",
    "preds_gmm = preds_gmm.withColumn(\"maxProb\", max_prob_udf(\"probability\")).withColumn(\"anomalyScore\", 1 - F.col(\"maxProb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4c901d6-c169-4584-840f-bea1522aded3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 265:=========>                                             (2 + 10) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== Anomaly Detection Metrics ==\n",
      "ROC AUC: 0.9492781251872102\n",
      "Precision@284: 0.2007042253521127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "roc_anom = ev_roc.evaluate(preds_gmm.withColumn(\"rawPrediction\", F.col(\"anomalyScore\")))\n",
    "total = preds_gmm.count()\n",
    "k = int(total * 0.001)\n",
    "topK = preds_gmm.orderBy(F.desc(\"anomalyScore\")).limit(k)\n",
    "prec_at_k = topK.filter(F.col(\"Class\") == 1).count() / k if k>0 else None\n",
    "print(\"\\n== Anomaly Detection Metrics ==\")\n",
    "print(\"ROC AUC:\", roc_anom)\n",
    "print(f\"Precision@{k}:\", prec_at_k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d1aef06-7073-4252-b89b-12d0fa8307df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 21:46:09 WARN Utils: Your hostname, Molphie resolves to a loopback address: 127.0.1.1; using 192.168.6.223 instead (on interface enp4s0)\n",
      "25/06/09 21:46:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/09 21:46:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName(\"IBM_HR_Attrition\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ff755e-6104-4825-9910-bee5d2d7de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"WA_Fn-UseC_-HR-Employee-Attrition.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8dc28c-ac8e-40de-84df-00b64c071f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_indexer = StringIndexer(inputCol=\"Attrition\", outputCol=\"label\")\n",
    "\n",
    "categorical_cols = [field for (field, dtype) in data.dtypes if dtype == \"string\" and field != \"Attrition\"]\n",
    "numeric_cols = [field for (field, dtype) in data.dtypes if ((dtype == \"double\") or (dtype == \"int\")) and field != \"EmployeeNumber\"]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_indexed\") for column in categorical_cols]\n",
    "\n",
    "assembler_inputs = [col + \"_indexed\" for col in categorical_cols] + numeric_cols\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7ef4c3-2ba4-41ac-bccf-bb885bc02e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[attrition_indexer] + indexers + [assembler])\n",
    "model = pipeline.fit(data)\n",
    "dataset = model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fd88669-8d61-486a-a25c-3324cb2f6c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = dataset.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4521402-45d8-4f29-9e9c-84e9bf1e82ed",
   "metadata": {},
   "source": [
    "# Классификация: предсказание увольнения сотрудников."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1e8ef3d-b99c-486e-b7a0-54ac80de59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "rf_model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42e82364-c610-4224-9419-e534a3717f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8546\n",
      "F1 Score: 0.8090\n"
     ]
    }
   ],
   "source": [
    "predictions = rf_model.transform(test_data)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1_score = f1_evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7db5da-ab1c-4f7f-8a22-a19fd90cc5e8",
   "metadata": {},
   "source": [
    "# Кластеризация: сегментация сотрудников по характеристикам.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34cb24ee-c298-4048-af6e-fb9e1ca251ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29cab980-4627-413e-809d-158e72f47180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/09 21:46:20 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(featuresCol=\"features\", k=3, seed=42)\n",
    "\n",
    "kmeans_model = kmeans.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d50ff29f-5fa2-4f52-b188-65cbbeacfe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.6757\n"
     ]
    }
   ],
   "source": [
    "cluster_predictions = kmeans_model.transform(dataset)\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(cluster_predictions)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0856a-61dc-4490-9559-22fad06bb517",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95360fc0-fad3-4482-b6d9-868843fdf8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95d56311-a62b-4102-8f3d-7ff0fc9b9b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "lr_model = lr.fit(train_data)\n",
    "\n",
    "lr_predictions = lr_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1dff2700-d14b-4784-98bd-f8f11eb83147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Logistic Regression] Accuracy: 0.8750\n",
      "[Logistic Regression] F1 Score: 0.8553\n",
      "[Logistic Regression] ROC AUC: 0.7999\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(lr_predictions)\n",
    "\n",
    "f1_eval = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = f1_eval.evaluate(lr_predictions)\n",
    "\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = binary_eval.evaluate(lr_predictions)\n",
    "\n",
    "print(f\"[Logistic Regression] Accuracy: {accuracy:.4f}\")\n",
    "print(f\"[Logistic Regression] F1 Score: {f1:.4f}\")\n",
    "print(f\"[Logistic Regression] ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b22e22a4-1d34-461f-88ef-7acf2769d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Top 10 Most Influential Features]\n",
      "+------------------------+--------------------+-------------------+\n",
      "|feature                 |coefficient         |abs_coeff          |\n",
      "+------------------------+--------------------+-------------------+\n",
      "|OverTime_indexed        |1.8828789117673954  |1.8828789117673954 |\n",
      "|Department_indexed      |0.5684795046190597  |0.5684795046190597 |\n",
      "|JobInvolvement          |-0.5473082779157762 |0.5473082779157762 |\n",
      "|StockOptionLevel        |-0.47789011787045166|0.47789011787045166|\n",
      "|Gender_indexed          |-0.47154221174564753|0.47154221174564753|\n",
      "|EnvironmentSatisfaction |-0.43561554380107337|0.43561554380107337|\n",
      "|JobLevel                |-0.41371868389716754|0.41371868389716754|\n",
      "|JobSatisfaction         |-0.33322899696410957|0.33322899696410957|\n",
      "|RelationshipSatisfaction|-0.2622509130405401 |0.2622509130405401 |\n",
      "|NumCompaniesWorked      |0.17351227663856217 |0.17351227663856217|\n",
      "+------------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "coef_array = lr_model.coefficients.toArray().tolist()\n",
    "coef_rows = [Row(feature=f, coefficient=c, abs_coeff=abs(c)) for f, c in zip(assembler_inputs, coef_array)]\n",
    "coef_sdf = spark.createDataFrame(coef_rows)\n",
    "\n",
    "print(\"\\n[Top 10 Most Influential Features]\")\n",
    "coef_sdf.orderBy(\"abs_coeff\", ascending=False).show(10, truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
